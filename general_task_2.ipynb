{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPa1X2yxj1rLRFdA+Z+btWm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Let's play with language!\n","\n","We know that computers are really good at number crunching; because of this, they're very often associated with areas like Maths and Physics.\n","\n","\n","\n","**But** computers can also be used to play with *language* in cool ways. You've probably encountered a lot of language based technologies already -- chat interfaces, like ChatGPT, and even more basic things like the predictive text function on your phone. In the world of Literature, scholars have develeoped a whole other way of thinking about texts using computation called *distant reading* (where they look at high-level trends as opposed to doing a *close reading*, which is a small or highly-targeted in-depth analysis).\n","\n","We're going to try our hand at using the computer to transform and even *generate* text!\n","\n","</br></br>\n","\n","\n","#### Getting some texts\n","\n","We're going to grab some text to work with from the internet. In text analysis, you often work with groups of realted documents which together form a *corpus*. Our corpus today is going to be collections of plays of different genres written by Shakespeare\n","\n","The below code fetches them and stores the data into a list of dictionaries. The keys of each dictionary are the play's title, its genre, and its full text. **You don't need to know how this code works**, but you are welcome to poke away at it to try to figure it out.\n","\n"],"metadata":{"id":"LIUgKRPLMEzp"}},{"cell_type":"code","source":["import os\n","!wget https://lexically.net/downloads/corpus_linguistics/ShakespearePlaysPlus.zip\n","!unzip ShakespearePlaysPlus.zip\n","\n","corpus = []\n","for genre in [\"comedies\", \"tragedies\", \"historical\"]:\n","  prefix = genre + \"/\"\n","  for f in os.listdir(genre):\n","    if f.endswith(\".txt\"):\n","      fi= open(prefix + f, encoding=\"utf-16-le\")\n","      corpus.append({\"title\":f[:-4], \"genre\":genre, \"text\":fi.read()})\n","\n","\n","#we expect this to be 37\n","print(len(corpus))\n","#and the titles of all of the plays\n","for doc in corpus:\n","  print(doc[\"title\"])\n"],"metadata":{"id":"B5qz0nIs8O34"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now if we take a look at one of the documents we just read in, we see that it is full of *tags* used to distinguish things like acts, characher names, and stage directions.\n","\n","\n"],"metadata":{"id":"5p2KAdx4tTeH"}},{"cell_type":"code","source":["#you can change the number in the first set of square brackets if you want to see different ones\n","print(corpus[17][\"text\"][:1000])"],"metadata":{"id":"4xq0qRsffZ6K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Tokenization\n","\n","\n","*We* know what all this means but the computer, dumb rock that it is, has no idea. Computers see everything as a series of 1s and 0s. The computer doesn't *know* what a stage direction is, what a character name is, or even what a *word* is. We have to teach it what collections of 1s and 0s matter to us.\n","\n","The process of dividing up the text into those meaningful collections -- which we'll call **tokens** is called *tokenization*. For today, we're going to use the **word** as the smallest unit we care about, so our tokens will be individual words. We're only going to care about words spoken as part of *dialogue*. This menas that we're going to discard any of the text enclosed in angled brackets.\n","\n","**Your first task is to write a function to tokenize a text**.\n","\n","Currently, the smallest unit python 'understands' is an individual character. That means that tokenization looks like looping over each character and deciding to either stick it to the end of a sequence of characters that will eventually make up a new word, or knowing that a given character indicates the end of a word and stopping the adding process (and adding the completed word to a running list of tokens).\n","\n","This means we have to brainstorm all of the scenarios that indicate the end of a word. We should probably start a new token if:\n","\n","- We hit a piece of puncutuation that *isn't* an apostrophe (sometimes you might to this differently, but for not we don't want posessives to be different tokens)\n","- We hit any sort of whitespace character (newlines, spaces)\n","\n","\n","In our particular case, we also need to account for angled brackets. Finally, we want to make sure that all of our tokens are only lowercase.\n","\n","</br>\n","\n","####A few helpful things to know\n","\n","* `String.puncutation` is a string that includes all types of punctuation.\n","* `String.whitespace` is a string that includes all types of whitespace characters.\n","* The .lower() string function makes any string (include one character strings) lowercase.\n","* You can use a simple for loop to loop over any character\n","\n","\n","\n","\n","*Check out the code below for some examples. Make sure to read the comments!*\n","\n","\n"],"metadata":{"id":"klQD4s8MfaJw"}},{"cell_type":"code","source":["\n","#getting string punctuation\n","import string\n","punct = string.punctuation\n","\n","#added the emdash because it was missing\n","punct += \"â€”\"\n","#keeping the apostrophe because we're not doing advanced tokenization so we'll want to keep contractions\n","punct = punct.replace(\"'\", \"\")\n","\n","\n","s = 'We are the knights who say \"Ni\"!'\n","#This does very basic tokenization (printing out each token)\n","#you'll need to add some things to account for angled brackets, and add all the tokens to a list instead of printing them\n","running = \"\"\n","new_token = False\n","for c in s:\n","  if c in punct:\n","    new_token = True\n","  elif c in string.whitespace:\n","    new_token = True\n","  if new_token:\n","    if len(running)>0:\n","      print(running.lower())\n","    new_token = False\n","    running =\"\"\n","  else:\n","    running += c\n","\n","\n","\n","\n"],"metadata":{"id":"QjxL8ceThwKu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Your turn!\n","\n","Fill in the tokenization function below. If do your tokenization about the same way we have, the test code should output `16601`\n","\n","Don't just rely on that, though! Make sure you print out some part of your list of tokens to make sure things are working as you expect!"],"metadata":{"id":"yizH81PaWbAk"}},{"cell_type":"code","source":["#This is a copy of one of the texts from the corpus, for us to play around with\n","sample = corpus[17][\"text\"][:]\n","\n","\n","def tokenize(text):\n","  #YOUR CODE HERE\n","\n","\n","sample_tokens = tokenize(sample)\n","print(len(sample_tokens))\n","\n","\n","#compare the start of the text to your tokeization\n","#print(sample[:1000])\n","print(sample_tokens)"],"metadata":{"id":"z4if4MUbXktl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Talking like Shakespeare\n","\n","\n","Now that we've sort of taught the computer what an individual work is, we're going to teach it how to string them together into sequences that (more of less) make sense.\n","\n","</br>\n","\n","**To start, your first task is to build a massive list of all the tokens used in ONLY THE COMEDIES**.\n","\n","You should use your tokenization code to help you do this. You should end up with a total number of tokens around `349361`"],"metadata":{"id":"4SGKMm-RexEX"}},{"cell_type":"code","source":["comedy_tokens = []\n","#YOUR CODE HERE"],"metadata":{"id":"FybtZ5LagSDQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Language is really complicated. There are all sorts of rules (and many *many* exceptions) that we've all internalised about what kinds of words go together.\n","\n","We could *try* to teach the computer all of those rules, but all the exeptions would make it tricky (and that's not even taking into account the fact that we're using antiquated language)! There's a lot of nuance that it would be hard to make concrete.\n","\n","Instead, we're going to find a way of 'teaching' the computer about which words go together through something computers are really good at: maths!\n","\n","We're going to take a huge number of examples (that massive token list we just created) and count up the number of times pairs of words appear beside eachother. These pair are called **bigrams**.\n","\n","**Your task is to write a function that creates a dictionary where the keys are bigrams, and the values are the number of times they appear.**\n","\n","If you do it like we did, your top ten (we give you some code to calculate and print this) should look something like:\n","\n","```\n","('i', 'am') 932\n","('i', 'will') 805\n","('i', 'have') 753\n","('in', 'the') 702\n","('to', 'the') 575\n","('of', 'the') 570\n","('it', 'is') 535\n","('my', 'lord') 500\n","('to', 'be') 453\n","('that', 'i') 430\n","```\n","\n"],"metadata":{"id":"aQCTTZFYhdaK"}},{"cell_type":"code","source":["#this is a helper function for you to use -> for this question and others later on\n","def top_ten(d):\n","  keys = list(d.keys())\n","  keys.sort(key=lambda x:d[x], reverse=True)\n","  for i in range(10):\n","    print(keys[i], d[keys[i]])\n","\n","\n","\n","def bigram_freq(tokens):\n","  #YOUR CODE HERE\n","\n","bigrams = bigram_freq(comedy_tokens)\n","top_ten(bigrams)"],"metadata":{"id":"4jbXsDm6jENh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These are pretty neat! Let's make things a little more complex, and try doing the same thing, but with sets of *three* words this time; these are called **trigrams**.\n","\n","*Your code will be pretty similar to the bigram code.*\n","\n","If you do it correctly, your top ten should look something like:\n","\n","```\n","('i', 'pray', 'you') 157\n","('i', 'will', 'not') 106\n","('i', 'know', 'not') 75\n","('i', 'am', 'not') 75\n","('i', 'am', 'a') 72\n","('i', 'do', 'not') 69\n","('it', 'is', 'a') 66\n","('and', 'i', 'will') 65\n","('there', 'is', 'no') 64\n","('i', 'would', 'not') 61\n","\n","```"],"metadata":{"id":"LwJOecIckvdI"}},{"cell_type":"code","source":["def trigram_freq(tokens):\n","  #YOUR CODE HERE\n","\n","trigrams = trigram_freq(comedy_tokens)\n","top_ten(trigrams)"],"metadata":{"id":"2_EfPyXhl9ju"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A cool thing that we can do with these bigram and trigram counts is calculate the probability of any single token appearing after two other tokens.\n","\n","To calculate this probability of token t and position i (`t[i]`) we simply do:\n","\n","```\n","trigram_frequency of ( t[i-2], t[i-1], t[i] ) / bigram frequency of (t[i-2], t[i-1])\n","```\n","\n","**Let's write a function that, for each DISTINCT token (in our comedy only corpus), will calculate the probability of it appearing following two other given tokens.**\n","\n","This function should return a dictionary where the keys are the distinct words, and each key's value is the probability value for that word.\n","As parameters, you need to give it two previous tokens that at some point appear beside eachother.\n","\n","</br>\n","\n","*A useful thing to know*: You can use python `sets` to get all the distinct elements in a list. For example, the follow code:\n","\n","```\n","l = [1,2,2,3,4]\n","dist = list(set(l))\n","print(dist)\n","```\n","outputs\n","\n","```\n","[1,2,3,4]\n","```\n","\n","If you test your distribution funcition with the previous tokens `\"i\"` and `will`, your top ten should look like:\n","\n","```\n","not 0.13167701863354037\n","be 0.06459627329192547\n","go 0.03354037267080745\n","tell 0.02732919254658385\n","do 0.024844720496894408\n","have 0.02236024844720497\n","make 0.01987577639751553\n","give 0.013664596273291925\n","never 0.013664596273291925\n","no 0.009937888198757764\n","\n","```"],"metadata":{"id":"znkKA_qVmnpW"}},{"cell_type":"code","source":["def trigram_freq_dist(distinct_tokens, bigram_freq, trigram_freq, prev_prev_token, prev_token):\n","  #YOUR CODE HERE\n","\n","\n","d_tokens = list(set(comedy_tokens))\n","#testing with 'i will'\n","dist = trigram_freq_dist(d_tokens, bigrams, trigrams, 'i', 'will')\n","top_ten(dist)"],"metadata":{"id":"e3G989BqnQ4h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The really cool thing that we can do now that we have this probability distribution is *hallucinate* text that sounds a bit like Shakespeare! Starting with two tokens, we can predict -- like we did above -- what token might come next. If we pick one of those tokens, we can now use that new token and the previous one to generate *another* prediction....and so on and so forth. This is similarto how predictive text works on your phone.\n","\n","To make things more interesting, instead of always picking the most proable token (which would give us the same text every time), we'll pseudorandomly pick in a way that takes the probability into account. The function below does just that, given the trigram distribution. Notice how the two different calls (likely) yield different results."],"metadata":{"id":"4ApBJMb92lVX"}},{"cell_type":"code","source":["import random\n","\n","#this function depends on at least one of the trigram probabilities being non-zero\n","#so you should make sure that's true before calling it\n","def pick_from_dist(tri_dist):\n","  vals = list(tri_dist.keys())\n","  choice = random.choices(vals, weights=tri_dist.values(), k=1)\n","  return choice[0]\n","\n","\n","trigram_dist = trigram_freq_dist(d_tokens, bigrams, trigrams, \"i\", \"have\" )\n","print(pick_from_dist(trigram_dist))\n","print(pick_from_dist(trigram_dist))"],"metadata":{"id":"fH13uqkt3B51"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Your task is to write a function that takes two starting tokens (as a list), the tokens, bigram and trigram frequencies, and an integer `n` and generates a text of length `n` that starts with those tokens.**\n","\n","For example, if you pass in the tokens `'i'`, `'will'` and the integer `20` you might end up with something like:\n","\n","```\n","i will not come fair princess he is to conjure tears up in a riot take your leave good madam\n","```\n","or\n","```\n","i will not trust you not hear her lamb when it bites and you shall see it in a quarrel\n","\n","```\n","\n","Before you start writing code, think carefully about the process. You might want to try writing down individual steps as comments and then gradually transforming those into code."],"metadata":{"id":"5LB-U7YH3anx"}},{"cell_type":"code","source":["def hallucinate(tokens,start_tokens, bi_f, tri_f, n):\n","  #YOUR CODE HERE\n","\n","\n","print(hallucinate(d_tokens, [\"i\", \"will\"], bigrams, trigrams, 20))"],"metadata":{"id":"NACCW1HA3XJZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Not entirely coherent, but *vaguely* shakesperian!\n","\n","This is operating uniquely on the text from the comedies. **If you wanted, you could try the same thing on the *entire* corpus, or on just the collections of tragedies or historical plays, to see if you get different results.**"],"metadata":{"id":"KLzFrUd36Gzk"}},{"cell_type":"markdown","source":["## Extension task: looking for meaning\n","\n","In the first part, we used the documents to *generate* text. Another thing we sometimes use computers for is trying to *understand* things about text, without reading it all. In literature, this use of computers and statistical methods to identify high-level trends is sometimes called *distant reading* (as a counterpoint to *close reading*).\n","\n","One basic technique that we can use to start to get an idea of what a document is talking about, is to (after tokenization) **count how many times each distinct word occurs**.\n","\n","\n","**Your first task is to write a function (staring with the give header) that will take a document's tokens and return a dictionary where the keys are distinct tokens and the values are integers correspinding to how many times each token appeared**. This is called a *frequency vector*.\n","\n","\n","If you do it correctly, your top 10 for *Macbeth* should look something like:\n","\n","```\n","the 700\n","and 515\n","to 398\n","of 333\n","i 312\n","that 229\n","a 217\n","you 203\n","in 198\n","my 192\n","```"],"metadata":{"id":"r5UiB-JLm25O"}},{"cell_type":"code","source":["def freq_vec(tokens):\n","  #YOUR CODE HERE\n","\n","\n","m_tokens = tokenize(corpus[17][\"text\"])\n","vector = freq_vec(m_tokens)\n","top_ten(vector)"],"metadata":{"id":"Zunn7Ry4hVlN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Now, try using the code you've written to print out the top ten for every TRAGEDY in the corpus**."],"metadata":{"id":"ZqNO9r7OFU0J"}},{"cell_type":"code","source":["#YOUR CODE HERE"],"metadata":{"id":"4X9wDYL3FqdS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Well that doesn't tell us much!**\n","\n","Looking at the frequency vectors for each play, we can see that they're mostly made up of words like `and`, `the` and `you` -- words that are used in every kind of text. In the world of natural language processing, these are called *stopwords*. In many problem-contexts, we *remove* them from token lists in order to get more meaningful results.\n","\n","</br>\n","\n","The below code creates a list of stopwords for you. You'll notice that it includes some modern variations that won't show up in Shakespeare, but it'll do. If you happen to notice antiquated forms popping up in our texts, you can try adding them in."],"metadata":{"id":"q7MyZAbnmBGv"}},{"cell_type":"code","source":["#NOTE: could use the NLTK stopwords instead, or read them in from a text file\n","#the reasons to do it as  would be 1) not importing and 2)they can see what the words are/play with them\n","\n","# Source is Table 2.1 of Chapter 2 of Information Retrieval by C.J. Van Rijsbergen\n","# http://www.dcs.gla.ac.uk/Keith/Chapter.2/Ch.2.html\n","\n","stopword_text = \"\"\"\n","A               CANNOT          INTO            OUR             THUS\n","ABOUT           CO              IS              OURS            TO\n","ABOVE           COULD           IT              OURSELVES       TOGETHER\n","ACROSS          DOWN            ITS             OUT             TOO\n","AFTER           DURING          ITSELF          OVER            TOWARD\n","AFTERWARDS      EACH            LAST            OWN             TOWARDS\n","AGAIN           EG              LATTER          PER             UNDER\n","AGAINST         EITHER          LATTERLY        PERHAPS         UNTIL\n","ALL             ELSE            LEAST           RATHER          UP\n","ALMOST          ELSEWHERE       LESS            SAME            UPON\n","ALONE           ENOUGH          LTD             SEEM            US\n","ALONG           ETC             MANY            SEEMED          VERY\n","ALREADY         EVEN            MAY             SEEMING         VIA\n","ALSO            EVER            ME              SEEMS           WAS\n","ALTHOUGH        EVERY           MEANWHILE       SEVERAL         WE\n","ALWAYS          EVERYONE        MIGHT           SHE             WELL\n","AMONG           EVERYTHING      MORE            SHOULD          WERE\n","AMONGST         EVERYWHERE      MOREOVER        SINCE           WHAT\n","AN              EXCEPT          MOST            SO              WHATEVER\n","AND             FEW             MOSTLY          SOME            WHEN\n","ANOTHER         FIRST           MUCH            SOMEHOW         WHENCE\n","ANY             FOR             MUST            SOMEONE         WHENEVER\n","ANYHOW          FORMER          MY              SOMETHING       WHERE\n","ANYONE          FORMERLY        MYSELF          SOMETIME        WHEREAFTER\n","ANYTHING        FROM            NAMELY          SOMETIMES       WHEREAS\n","ANYWHERE        FURTHER         NEITHER         SOMEWHERE       WHEREBY\n","ARE             HAD             NEVER           STILL           WHEREIN\n","AROUND          HAS             NEVERTHELESS    SUCH            WHEREUPON\n","AS              HAVE            NEXT            THAN            WHEREVER\n","AT              HE              NO              THAT            WHETHER\n","BE              HENCE           NOBODY          THE             WHITHER\n","BECAME          HER             NONE            THEIR           WHICH\n","BECAUSE         HERE            NOONE           THEM            WHILE\n","BECOME          HEREAFTER       NOR             THEMSELVES      WHO\n","BECOMES         HEREBY          NOT             THEN            WHOEVER\n","BECOMING        HEREIN          NOTHING         THENCE          WHOLE\n","BEEN            HEREUPON        NOW             THERE           WHOM\n","BEFORE          HERS            NOWHERE         THEREAFTER      WHOSE\n","BEFOREHAND      HERSELF         OF              THEREBY         WHY\n","BEHIND          HIM             OFF             THEREFORE       WILL\n","BEING           HIMSELF         OFTEN           THEREIN         WITH\n","BELOW           HIS             ON              THEREUPON       WITHIN\n","BESIDE          HOW             ONCE            THESE           WITHOUT\n","BESIDES         HOWEVER         ONE             THEY            WOULD\n","BETWEEN         I               ONLY            THIS            YET\n","BEYOND          IE              ONTO            THOSE           YOU\n","BOTH            IF              OR              THOUGH          YOUR\n","BUT             IN              OTHER           THROUGH         YOURS\n","BY              INC             OTHERS          THROUGHOUT      YOURSELF\n","CAN             INDEED          OTHERWISE       THRU            YOURSELVES\n","\"\"\"\n","\n","stopwords = set( [ s.lower().strip() for s in stopword_text.strip().split() ] )\n","print(stopwords)"],"metadata":{"id":"W4cxa4JsnUaZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Try running the code you wrote to calculate and display the frequency vectors for each tragedy but, this time, **remove all stopwords from the token list first**.\n","\n","Your new top 10 for *Romeo and Juliet* should look like:\n","\n","```\n","thou 277\n","thy 166\n","o 150\n","love 137\n","thee 137\n","romeo 113\n","shall 110\n","come 97\n","do 89\n","good 86\n","```"],"metadata":{"id":"_T57VOCRnVDz"}},{"cell_type":"code","source":["#YOUR CODE HERE"],"metadata":{"id":"E141ZYX3unGD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","We're still seeing some common oldey-timey words in there, but this now gives us a better sense of the topics of each document.\n","\n","That said, a lot of the documents share most frequent words. This means that t can be hard to tell which words are the most important to a *specific* document.\n","\n","To address this challenge, we can use something called **TF-IDF** which stands for **Term Frequency - Inverse Document Frequency**. What TF-IDF lets us do is figure out which words in a document are most important (a combination of frequent and unique) to an individual document with respect to the corpus overall.\n","\n","We calculate the individual TF-IDF value of a token using the following formula:\n","\n","```\n","TF * IDF\n","```\n","\n","Where **TF** is\n","\n","```\n","TF(t,d) = number of times token t appears in document d/total number of tokens in document d\n","```\n","and **IDF** is\n","\n","```\n","IDF(t) = log(number of documents in the corpus/number of corpus documents which use token t\n","```\n","\n","\n","**Your job is to calculate the TF-IDF vectors for each of the documents in the corpus**. This means claculating the individual values for each of its unique tokens, like you did for frequency vectors. You should be using your (stopword-free) frequency vectors as part of this process. Note that you can use `math.log10(x)` to take the base 10 log of a value `x`.\n","\n","\n","To make your life easier, the code below creates a dictionary there the keys are all the tokens (words) used in the overall corpus, and the values are the number of corpus documents in which that word appears."],"metadata":{"id":"GKuLhWPl-QNn"}},{"cell_type":"code","source":["def create_corp_counts():\n","  usage = {}\n","  for c in corpus:\n","    if c[\"genre\"]=='tragedies':\n","      no_stop = [x for x in tokenize(c[\"text\"]) if x not in stopwords]\n","      unique = set(no_stop)\n","      for word in unique:\n","        if word not in usage:\n","          usage[word]=0\n","        usage[word] +=1\n","  return usage\n","\n","doc_usage = create_corp_counts()\n","print(doc_usage)\n"],"metadata":{"id":"hQG8o5SWDRiX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We also know that there are **10** documents in our corpus (if we're only looking at tragedies. But don't take our word for it, run the code below to confirm!"],"metadata":{"id":"NGLgwX6gQpF4"}},{"cell_type":"code","source":["num =0\n","for c in corpus:\n","  if c[\"genre\"]==\"tragedies\":\n","    num +=1\n","\n","print(num)"],"metadata":{"id":"r0qSvxR5Q4sr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now you should have all the pieces you need to write the tf-idf vectorization code. **Write your code in the cell below**.\n","\n","If you so it correctly, your updated top 10 for *Romeo and Juliet* should look something like:\n","\n","```\n","romeo 0.00916240979485932\n","tybalt 0.0037298305359604314\n","juliet 0.0032433309008351578\n","friar 0.0017838319954593368\n","montague 0.0017027487229384578\n","paris 0.0013601946082919365\n","county 0.0012973323603340632\n","mercutio 0.0011351658152923053\n","thursday 0.0011351658152923053\n","capulet 0.0010540825427714264\n","```"],"metadata":{"id":"UZ-Q3e0PRKqW"}},{"cell_type":"code","source":["import math\n","def tfidf_vectorize(freq_vec, num_docs, doc_use):\n","  #YOUR CODE HERE\n","\n","\n","no_stop = [x for x in tokenize(corpus[18][\"text\"]) if x not in stopwords]\n","f = freq_vec(no_stop)\n","tfidf = tfidf_vectorize(f, 10, doc_usage)\n","top_ten(tfidf)"],"metadata":{"id":"Hm5g0tDuIp18"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Now try calculating and outputting the tf-idf top 10 for each tragedy, just like you did for the frequency vectors**."],"metadata":{"id":"3alKLoQyUB8k"}},{"cell_type":"code","source":["#YOUR CODE HERE"],"metadata":{"id":"yfS7AI2NUPtk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you can see, it mostly ends up being character names and locations, but that certainly tells us a lot more about each specific document than normal freuquency vectors!\n","\n","If you want to get a sense of play topics and themes, you could try also designating the character names as stopwords, so that they'll be removed (or, more simply, just look beyond the top 10)!"],"metadata":{"id":"JhcfHJSmUoWe"}}]}